---
title: "Data Science with R"
author: "Yudum Paçin"
date: "14 October 2022"
output: 
  html_notebook:
    toc: true
    toc_depth: 3
    toc_float: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## **About Me**

Hello everyone! My names is Yudum Paçin. I was born in Bursa, Mudanya. I have been working as an IT Business Analyts in VakifBank for 6 years. However, I am about to switch my career to be a Data Scientist. My interest of data science area started from 2019 with the famous Coursera course "Maching Learning" from Andrew Ng, which i think no longer exists as one course but is converted to a Machine Learning Specialization. 

I was graduated from Mathematics department of  Middle East Technical University in 2012. After graduation, i was sure that my profession will be in the IT sector.For this reason, to improve my knowledge about software development, i started my Master degree in Informatics Institute, METU. As a data science enthusiast and future-data scientist, i want to use machine learning skills to automate repetitive jobs like document classification, e-mail replying, classification of incident records from production environments testing user interfaces etc. Even, reporting to manager can be a machine learning idea for me. To sum up, if there is a work that computers can handle better or equal than humans, then we should find a new way to automate it. 

You can reach me via my [linkedin](https://www.linkedin.com/in/yudum-pacin/) page.

## **Advocating for Automation**

This section is from the RStudio talk **"Advocating for Automation: Adapting Current Tools in Environmental Science through R"** by Hannah Podzorski, GSI Envronmental. Podzorski points out the importance of automation for diverse skilled teams and how they applied it with R. You can reach to talk from [link](https://www.rstudio.com/conference/2022/talks/advocating-for-automation/)

The reason I chose this topic is that I am also trying to find a new ways to automate the repetitive office works and create more time for more valuable jobs.
But, where to start automation? Podzorski suggest that start small, and she and team members decide to start to automate the process with Microsoft Excel Products.

`{openxlsx}` : this package is designed for edit, read and create Excel files in R. Once the programmer start to use openxlsx, it saved more time for analysts to analyze rather than  editing the csv file.

`openxlsx::write.xlsx(data,"data.xlsx")`

The other useful packages is `{officer}` and `{rvg}`.  Officer is used to manuplate word documents and PowerPoints in R. Rvg has a function called**dml** which enables the edit ggplot object option before exporting it to a PowerPoint file.

The team of Podzorski later decides to use their automation skills with ProUCL, a statistical software for Left Censored Enviromental data.The team loads the data as input and export the results for further analysis. The team decides to autamate this process with "Mini-Mouse Macro". First a function in R, subsets the large data in more managable chunks, and then the mouse macro takes these input files, and clicks though the software app. for getting statistics and saves the output file to PC. This can be seen as a small task to automate but when the number of files increase, the time for a human to do it one by one becomes tedious. Also, they automate the work of copy-paste ProUCL outputs to Excel. Finally, the team were able to save more time to focuse on data analysis. 

## **3 R Posts**

In this section, 3 R posts I chose will be summirized and discussed.


### **1. PCA vs Autoencoders for Dimensionality Reduction**

*You can reach the full version of article from the [link](https://www.r-bloggers.com/2018/07/pca-vs-autoencoders-for-dimensionality-reduction/)*

When our data set has too many dimensions, it is a wise desicion to go on with the important ones and leave others. But how to choose important ones? This article compares two  methods for dimension reduction, PCA and Autoencoders with R using the Australian Institute of Sport data set.

#### Principal Components Analysis (PCA)

PCA reduces the data frame by orthogonally transforming the data into a set of principal components. The first principal component explains the most amount of the variation in the data in a single component, the second component explains the second most amount of the variation, etc. By choosing the top k principal components that explain say 80-90% of the variation, the other components can be dropped since they do not significantly benefit the model. 

```
# standardise
minmax <- function(x) (x - min(x))/(max(x) - min(x))
x_train <- apply(ais[,1:11], 2, minmax)
# PCA
pca <- prcomp(x_train)
```
#### Autoencoder

The autoencoder will be constructed using the keras package. As with any neural network there is a lot of flexibility in how autoencoders can be constructed such as the number of hidden layers and the number of nodes in each. With each hidden layer the network will attempt to find new structures in the data. In general autoencoders are symmetric with the middle layer being the bottleneck. The first half of the autoencoder is considered the encoder and the second half is considered the decoder.

```
# set model
model <- keras_model_sequential()
model %>%
  layer_dense(units = 6, activation = "tanh", input_shape = ncol(x_train)) %>%
  layer_dense(units = 2, activation = "tanh", name = "bottleneck") %>%
  layer_dense(units = 6, activation = "tanh") %>%
  layer_dense(units = ncol(x_train))
```
To summarise, the key differences for consideration between PCA and autoencoders are:

* There are no guidelines to choose the size of the bottleneck layer in the autoencoder unlike PCA. 

* The autoencoder tends to perform better when k is small when compared to PCA.

* Autoencoders require more computation than PCA. 

### **2. Audio classification with torch**
In this article classification of audio using R torch is examined. I have tried audio classification with spectograms using Keras tensorflows with Python before, so this topic got my attention.

*You can reach the full version of article from the [link](https://blogs.rstudio.com/ai/posts/2022-10-06-audio-classification-torch/)*

The dataset for this study holds recordings of thirty different one- or two-syllable words, uttered by different speakers. There are about 65,000 audio files.

Waveform representation of sound files is actually difference of loudness of voice over time. To get more meaningful transformation, fourier-transform method is applied for getting a representation of sound in a way that had no information about time at all and have as much just as much information as original signal.  In R  ```torch_fft_fft``` function is used, where fft stands for Fast Fourier Transform.

We can divide the signal into small chunks, and run the Fourier Transform on each of them. This representation is  called the spectrogram.

The spectrogram is a two-dimensional representation: an image. From now on, we can use convolutional neural networks for image recognition using `library(torch)`.

### **3. Update Your Machine Learning Pipeline With vetiver and Quarto**

The reason I chose this article is I am curious about how the models are created and deployed in real-life. How is the Machine Learning Pipeline process works?

*You can reach the full version of article from the [link](https://www.rstudio.com/blog/update-your-machine-learning-pipeline-with-vetiver-and-quarto/)*

Machine learing operations (MLOps) are a set of practices for running ML models in production environments. Vertier, an open-source framework for entire morel life cycle, provides R an and Pyhton developers a unified way for working with ML models.

The app provides real-time predictions of the number of bikes at stations across the city, Washington D.C.. The end to end machine learning pipeline uses R to modify and import the data, saves it in a pin, which is a package publishes data, models, and other R objects. Then the pipeline developes a model and moves the model to a deployable location.

Creating An End-to-End Machine Learning Pipeline

1. Create a custom package for pulling data
2. Extract, transform, load process in R
3. Tidy and join datasets
4. Train and deploy the model
5. Create a model card
6. Monitor model metrics
7. Deploy a Shiny app that displays real-time predictions
8. Create project dashboard

<center>

<p style="font-size: 20pt; font-style: italic; color:#FF69B4">
Thanks for reading
</p>

</center>
